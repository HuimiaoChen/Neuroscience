# Hidden Neurons Modeling-Related Paper Reading

## Towards a theory of cortical columns: From spiking neurons to interacting neural populations of finite size

### Knowledge

#### Optogenetics

Optogenetics is a technique in neuroscience that uses light to control the activity of neurons in living tissue. This is achieved by introducing into the neurons a gene for a light-sensitive ion channel or pump that can be activated or inhibited by light. When the gene is expressed and the neurons are exposed to light, the ion channel or pump is activated or inhibited, which leads to a change in the membrane potential of the neuron and, thus, its activity. The main advantage of optogenetics is that it allows for very precise control over the activity of individual neurons or populations of neurons. 

#### Local field potential (LFP)

LFP stands for Local field potential. Local field potentials are voltage signals that are recorded from electrodes in the vicinity of neurons. They are typically measured from extracellular electrodes, and reflect the summed synaptic activity of many neurons in a particular brain region.

LFP signals can provide valuable information about the activity of neural circuits and can be used to study a variety of phenomena, including oscillations, rhythmic activity, and synchronized neural activity. The LFP can also be used to identify the presence of particular frequency ranges that are associated with different cognitive or motor processes, and to study the dynamics of neural activity over time.

LFP signals are often recorded in conjunction with other neurophysiological measures, such as single-unit activity and spikes, to provide a more complete picture of the neural activity in a given brain region. In recent years, the use of LFP recordings has become increasingly widespread in neuroscience research, and LFP signals are now commonly used to study the brain activity of animals and humans in a wide range of paradigms.

#### "Non-perturbative"

"Non-perturbative" refers to a method or approach in physics, mathematics, or theoretical science that does not rely on perturbation theory, meaning it does not make small or incremental changes to a system to obtain a solution. Instead, it aims to solve a problem in its entirety without relying on approximations or small corrections. Non-perturbative methods are often used to study strong interactions or quantum phenomena where perturbation theory may not be applicable.

#### Escape noise

"Escape noise" refers to a type of neural noise that is thought to play a role in generating spontaneous transitions from one state to another, such as from a resting state to an active state, or from one neural pattern to another.

Escape noise is thought to be generated by fluctuations in the membrane potential or in the activity of ion channels that are not related to any external stimulus, but instead reflect the inherent randomness of molecular processes in the neurons. These fluctuations can cause the membrane potential to cross a threshold, leading to the initiation of an action potential and a change in the state of the neuron. The term "escape noise" is used to describe this process because the fluctuations are thought to enable the neuron to "escape" from its current state and transition to a new one.

Escape noise is an important aspect of neural dynamics, as it contributes to the variability and unpredictability of neural activity, and has been proposed to play a role in the generation of rhythmic patterns, the formation of memories, and the computation of sensory information. Understanding the nature and effects of escape noise is an active area of research in neuroscience.

#### Synaptically-filtered background noise

"Synaptically-filtered background noise" refers to the variability in neural activity that results from the interaction of neurons through synaptic connections.

In a neural network, the activity of one neuron can influence the activity of other neurons through the release of neurotransmitters at synapses. This interaction can result in fluctuations in the membrane potentials and spiking patterns of the neurons, even in the absence of any external stimuli. This variability in activity is referred to as background noise.

The concept of synaptically-filtered background noise refers to the idea that the properties of synaptic connections can shape the background noise in a way that filters out certain frequencies or patterns and enhances others. For example, the strength, time course, and specificity of synaptic connections can all influence the background noise in a neural network.

The concept of synaptically-filtered background noise is important for understanding the ways in which neural networks process and transmit information, as well as for understanding the role of synaptic plasticity in shaping neural activity and learning.

#### Poisson distribution and Poisson process

The characteristic function of a probability distribution is defined as the expected value of the complex exponential function of the random variable $X$:

$$
\phi_X(t) = E[e^{itX}]
$$

To derive the characteristic function of the Poisson distribution, we start by calculating its probability mass function:

$$
P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!}
$$

where $\lambda$ is the mean of the distribution, $k$ is the number of random event occurrences per unit time.

Using the definition of the characteristic function, we have:

$$
\phi_X(t) = \sum_{k=0}^{\infty} e^{itk} P(X=k)
$$

Substituting in the expression for $P(X=k)$, we get:

$$
\phi_X(t) = \sum_{k=0}^{\infty} e^{itk} \frac{\lambda^k e^{-\lambda}}{k!}
$$

Simplifying the exponent, we have:

$$
\phi_X(t) = e^{-\lambda} \sum_{k=0}^{\infty} \frac{(\lambda e^{it})^k}{k!}
$$

Using the Taylor series expansion of $e^x$, we can write:

$$
\phi_X(t) = e^{-\lambda} e^{\lambda e^{it}} = e^{\lambda(e^{it}-1)}
$$

The characteristic function has several important properties, including the fact that its $n$th derivative evaluated at $t=0$ is equal to the $n$th moment of the distribution:

$$
\mu_n = E[X^n] = i^{-n} \frac{d^n}{dt^n} \phi_X(t) \biggr|_{t=0}
$$

This means that we can use the characteristic function to calculate the moments of the Poisson distribution by taking derivatives of its characteristic function and evaluating them at $t=0$.

For Poisson distribution, we have
$$
\phi'(t) = i\lambda e^{it}\cdot e^{\lambda(e^{it}-1)} = i\lambda e^{it}\cdot \phi(t)
$$

$$
\phi''(t) = -\lambda^2 e^{it} \cdot e^{\lambda(e^{it}-1)} + i\lambda e^{it}\cdot i\lambda e^{it} \cdot e^{\lambda(e^{it}-1)} = (\lambda^2 - \lambda)e^{it}\cdot \phi(t)
$$

$$
\phi'(0) = i\lambda\cdot \phi(0) = i\lambda
$$

$$
\phi''(0) = (\lambda^2 - \lambda)\cdot \phi(0) = \lambda^2 - \lambda
$$

An inhomogeneous Poisson process is a stochastic process in which events occur randomly in time, but the rate at which events occur is not constant. The rate function Î»(t) specifies the probability of an event occurring in a small interval of time around t. The inhomogeneous Poisson process is useful for modeling a variety of real-world phenomena, such as the arrival of customers in a store, the firing of neurons in the brain, and the occurrence of earthquakes. The probability distribution of the time intervals between successive events in an inhomogeneous Poisson process is not exponential, but can still be described using the properties of the Poisson process.

The probability of observing $n$ events in a time interval $[0,T]$ for an inhomogeneous Poisson process with rate function $\lambda(t)$ is given by:

$$
P(N_T = n) = \frac{(\int_0^T \lambda(t) dt)^n}{n!} e^{-\int_0^T \lambda(t) dt} \prod_{i=1}^n \frac{\lambda(t_i)}{\int_0^T \lambda(t) dt}
$$

where $t_i$ is the time of the $i$-th event, and $N_T$ is the number of events in the time interval $[0,T]$.

The expected value of the number of events in a time interval $[0,T]$ is:

$$
E[N_T] = \int_0^T \lambda(t) dt
$$

The variance of the number of events in a time interval $[0,T]$ is:

$$
Var[N_T] = \int_0^T \lambda(t) dt
$$

where $\lambda(t)$ is the rate function, which can be a function of time or some other variable.

#### Autocorrelation

Autocorrelation of deterministic signals: In signal processing, the autocorrelation function (ACF) is often used without the normalization, that is, without subtracting the mean and dividing by the variance. When the autocorrelation function is normalized by mean and variance, it is sometimes referred to as the autocorrelation coefficient or autocovariance function.

Autocorrelation of continuous-time signal: given a signal $f(t)$, the continuous autocorrelation $R_{ff}(\tau)$ is most often defined as the continuous cross-correlation integral of $f(t)$ with itself, at lag $\tau$.
$$
R_{ff}(\tau) = \int_{-\infty}^{\infty}f(t+\tau)\overline{f(t)}\,\mathrm{d}t=\int_{-\infty}^{\infty}f(t)\overline{f(t-\tau)}\,\mathrm{d}t = \left.(f(t) * \overline{f(-t)})\right|_{t=\tau}
$$
where $\overline{f(t)}$ represents the complex conjugate of $f(t)$ and $*$ represents the convolution operation. Note that the parameter $t$ in the integral is a dummy variable and is only necessary to calculate the integral. It has no specific meaning.

Auto-correlation of discrete-time signal: the discrete autocorrelation $R$ at lag $l$ for a discrete-time signal $y(n)$ is
$$
R_{yy}(l) = \sum_{n \in Z} y(n)\overline{y(n-l)}
$$
The above definitions work for signals that are square integrable, or square summable, that is, of finite energy. Signals that "last forever" are treated instead as random processes, in which case different definitions are needed, based on expected values. For wide-sense-stationary random processes (see the following text), the autocorrelations are defined as
$$
R_{ff}(\tau) = \mathrm{E}\left[f(t)\overline{f(t-\tau)}\right]\\
R_{yy}(l) = \mathrm{E}\left[y(n)\overline{y(n-l)}\right]
$$

For white noise, the autocorrelation is 
$$
R_{nn}(\tau)=\mathrm{E}\left.[n(t)n(t-\tau)\right.]=\delta(\tau)
$$
For cross-correlation, it is similar, just replacing the second $f$ function in above formulas with another function $g$.

**Wide-sense stationary (WSS) process**:

A wide-sense stationary (WSS) process is a type of random process (or stochastic process) with specific statistical properties that remain constant over time. In a WSS process, the first and second-order moments (mean and autocorrelation) do not depend on the absolute time but only on the time difference.

A random process $X(t)$ is considered wide-sense stationary if it satisfies the following two conditions:

1. Constant mean: The mean or expected value of the process remains constant over time. Mathematically, this is expressed as:

   $E[X(t)] = \mu$, for all $t$

   where $E[X(t)]$ denotes the expected value of the process at time $t$ and $\mu$ is a constant.

2. Time-invariant autocorrelation: The autocorrelation function of the process depends only on the time difference ($Ï = t_2 - t_1$) and not on the absolute time values. Mathematically, this is expressed as:

   $R_X(t_1, t_2) = E[X(t_1)\overline{X(t_2)}] = R_X(Ï)$

   where $R_X(t_1, t_2)$ is the autocorrelation function, $\overline{X(t_2)}$ denotes the complex conjugate of $X(t_2)$, and $Ï = t_2 - t_1$.

A wide-sense stationary process is a simpler form of stationarity compared to strict-sense stationarity, which requires all the statistical properties of the process to be time-invariant. WSS processes are often used in the analysis of random signals, as their time-invariant autocorrelation function and constant mean make them more tractable for mathematical analysis and practical applications, such as filtering and system identification.

#### Power spectrum and power spectral density

Refer to this [video](https://www.bilibili.com/video/BV1E44y1D7mw/?spm_id_from=333.880&vd_source=967f24091b4dff8d77184edf51434a3c)'s first several minutes.

Note: understanding the orthogonality property of complex exponentials is essential when working with the power spectrum and power spectral density (PSD). The orthogonality property states that the inner product of two complex exponential functions with different frequencies is zero, while the inner product of a complex exponential function with itself is non-zero. Mathematically, this can be expressed as:

$â«_{-â}^{â} e^{j2ÏÎ½t} Â· e^{-j2ÏÎ½' t} dt = Î´(Î½ - Î½')$

Where $Î½$ and $Î½'$ are the frequencies of the complex exponentials, and $Î´(Î½ - Î½')$ is the Dirac delta function.

**Wiener-Khinchin theorem**:

The theorem establishes a relationship between the autocorrelation function of a wide-sense stationary random process (see [Autocorrelation](#Autocorrelation)) and its power spectral density.

The Wiener-Khinchin theorem states that the autocorrelation function $R(Ï)$ and the power spectral density $S(Ï)$ of a wide-sense stationary random process are a Fourier transform pair. In other words, the power spectral density is the Fourier transform of the autocorrelation function, and the autocorrelation function is the inverse Fourier transform of the power spectral density.

Mathematically, the Wiener-Khinchin theorem can be expressed as:

$S(Ï) = â«_{-â}^{â} R(Ï) e^{-jÏÏ} dÏ$

and

$R(Ï) = (1/2Ï) â«_{-â}^{â} S(Ï) e^{jÏÏ} dÏ$

It's important to note that $R(0)$ has a specific meaning in the context of PSD. $R(0)$ represents the total power (or energy, for energy signals) of the signal, which is equal to the integral of the PSD over all frequencies:

$R(0) = â«_{-â}^{â} S(Ï) dÏ$

However, this does not mean that $Ï = 0$ is required when calculating the PSD from the autocorrelation function. But for a white noise, $R(\tau)=\mathrm{E}\left.[n(t)n(t-\tau)\right.]=\delta(\tau)$, that is, $R(\tau)=0$ if $\tau \neq 0$.

(What is the definition of the PSD? It should not be related to Winer-Khinchin theorem. Then how to understand the  Winer-Khinchin theorem. from the perspective of the definition.)

#### Parseval's theorem


### Views

â 